{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('../../Models')\n",
    "from models import LSTMPredictor\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "load_dotenv()\n",
    "cluster_uri = os.environ.get(\"MONGODB_URI\")\n",
    "client = MongoClient(cluster_uri)\n",
    "db = client[\"MSCI446_DB\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, target, sequence_length):\n",
    "    sequences = []\n",
    "    target_sequences = []\n",
    "    for i in range(len(features) - sequence_length):\n",
    "        sequences.append(features[i:i+sequence_length])\n",
    "        target_sequences.append(target[i+1:i+1+sequence_length])\n",
    "   \n",
    "    sequences = np.array(sequences, dtype=np.float32)\n",
    "    target_sequences = np.array(target_sequences, dtype=np.float32)\n",
    "    \n",
    "    return torch.from_numpy(sequences), torch.from_numpy(target_sequences)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0  \n",
    "        for seq, targets in train_loader:\n",
    "            seq, targets = seq.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(seq).to(device)\n",
    "            output = output.unsqueeze(-1)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)  # Store the average loss for this epoch\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss}')\n",
    "    return train_losses\n",
    "\n",
    "def test_model(model, test_loader, criterion, device, future=0):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    test_losses = []\n",
    "    total_loss = 0\n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, targets in test_loader:\n",
    "            seq, targets = seq.to(device), targets.to(device)\n",
    "            output = model(seq, future=future).to(device)\n",
    "            if future > 0:\n",
    "               \n",
    "                extended_targets = torch.cat([targets, torch.zeros(targets.size(0), future, 1, device=device)], dim=1)\n",
    "            else:\n",
    "                extended_targets = targets\n",
    "            \n",
    "           \n",
    "            output = output.unsqueeze(-1)\n",
    "            loss = criterion(output, extended_targets)\n",
    "            total_loss += loss.item()\n",
    "            test_losses.append(loss.item())\n",
    "            predictions.append(output.cpu())\n",
    "            actuals.append(extended_targets.cpu())\n",
    "            index += 1\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_loss}')\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    return test_losses, predictions, actuals\n",
    "\n",
    "def predict_future(model, input_sequence, device, future_steps=1):\n",
    "    model.eval()  \n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for seq, targets in input_sequence:\n",
    "            seq, targets = seq.to(device), targets.to(device)\n",
    "            pred = model(seq, future=future_steps)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        \n",
    "      \n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def print_metrics(actuals, predictions):\n",
    "    r2 = r2_score(actuals.squeeze(-1).numpy(), predictions.squeeze(-1).numpy())\n",
    "    mae = mean_absolute_error(actuals.squeeze(-1).numpy(),predictions.squeeze(-1).numpy())\n",
    "    print(f'RÂ² Score: {r2}')\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "def plot_predictions(predictions, actuals, num_sequences=5):\n",
    "    for i in range(min(num_sequences, predictions.size(0))):\n",
    "        df = pd.DataFrame(data={\n",
    "            'Time Step': list(range(predictions.size(1))) * 2,\n",
    "            'Value': torch.cat((actuals[i, :, 0], predictions[i, :, 0]), dim=0).numpy(),\n",
    "            'Type': ['Actual'] * predictions.size(1) + ['Predicted'] * predictions.size(1)\n",
    "        })\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.title(f'Sequence {i+1}')\n",
    "        sns.lineplot(x='Time Step', y='Value', hue='Type', style='Type', markers=True, dashes=False, data=df)\n",
    "        plt.legend(title='Type')\n",
    "        plt.show()\n",
    "    \n",
    "def plot_losses(train_losses, test_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_loss(train_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/Final_table.csv')\n",
    "data.head(20)\n",
    "\n",
    "# collection_merged = db[\"Merged\"]\n",
    "# data = pd.DataFrame(list(collection_merged.find()))\n",
    "# data = data.drop(columns=['_id'])\n",
    "# data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features and Target Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop(columns=['datetime_beginning_ept', 'DPL_historical_da']))\n",
    "scaled_target = scaler.fit_transform(data[['DPL_historical_da']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(scaled_features, scaled_target, sequence_length=78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_timesteps, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape((n_samples * n_timesteps, n_features))\n",
    "y_train_reshaped = y_train.reshape((n_samples * n_timesteps,))\n",
    "\n",
    "\n",
    "model_fr = SelectKBest(score_func=f_regression, k=5)\n",
    "\n",
    "data_new_fr = model_fr.fit(X_train_reshaped, y_train_reshaped)\n",
    "\n",
    "print(\"f_regression: \", data_new_fr.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_best = model_fr.transform(X_train_reshaped)  \n",
    "X_test_best = model_fr.transform(X_test.reshape((X_test.shape[0] * X_test.shape[1], X_test.shape[2])))\n",
    "\n",
    "X_train_best = X_train_best.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
    "X_test_best = X_test_best.reshape((X_test.shape[0], X_test.shape[1], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Valid Pytorch Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_best = TensorDataset(torch.from_numpy(X_train_best), y_train)\n",
    "train_loader_best = DataLoader(train_dataset_best, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset_best = TensorDataset(torch.from_numpy(X_test_best), y_test)\n",
    "test_loader_best = DataLoader(test_dataset_best, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_best = LSTMPredictor(input_features=X_train_best.shape[2], n_hidden=51, output_features=1).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_adam = optim.Adam(model_best.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "train_losses_best = train_model(model_best, train_loader_best, criterion, optimizer_adam, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Train Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_losses_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses_best, predictions_best, actuals_best = test_model(model_best, test_loader_best, criterion, device, future=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(actuals_best, predictions_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions_best, actuals_best, num_sequences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Future Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_values = predict_future(model_best, test_loader_best, device, future_steps=10)\n",
    "future_predictions = future_values[:, -10:]\n",
    "\n",
    "future_predictions_2d = future_predictions.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform to get back to the original dollar values\n",
    "unscaled_future_predictions = scaler.inverse_transform(future_predictions_2d)\n",
    "\n",
    "# Reshape back to the original shape with future predictions\n",
    "unscaled_future_predictions = unscaled_future_predictions.reshape(future_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, future_values in enumerate(unscaled_future_predictions):\n",
    "    print(f\"Future Values for Sequence {i+1}:\")\n",
    "    print(future_values)\n",
    "    print(\"\\n\")  # Add extra newline for readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
